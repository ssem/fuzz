#!/usr/bin/env python
import os
import urllib
from urllib2 import *

class Http_Client:
    def _init(self):
        self.content = ''
        self.header = ''
        self.url = ''
        self.header = ''
        self.code = 0
        self.error = ''

    def set_properties(self, header_info):
        self._header_info = header_info
        self._opener = build_opener()

    def get(self, url, data, timeout):
        self._init()
        try:
            if type(data) == dict: data = urllib.urlencode(data)
            elif data != None: data = urllib.quote_plus(data)
            request = Request(str(url), data, self._header_info)
            response = self._opener.open(request, timeout=timeout)
            self.content = str(response.read())
            self.header = str(response.info())
            self.url = str(response.geturl())
            self.code = int(response.code)
        except Exception as error:
            try:
                self.error = error.reason
            except:
                self.error = error

class Scrape:
    def run(self):
        urls = []
        downloads = []
        http = Http_Client()
        http.set_properties({'User-Agent':'Mozilla/5.0',
            'Cookie':'Apache::Net2Net::AuthCookie_NTNWeb=steven_sem3@hotmail.com`bb2a36d870ba4f13185ca02e93bf0619; region=amer; email=steven_sem3%40hotmail.com; first_name=steve; last_name=sem; name=steve%20sem; country=US'})
        http.get('http://www.zhone.com/support/downloads/cpe/', None, 3)
        for line in http.content.split('\n'):
            if 'href="/support/downloads/cpe' in line:
                urls.append('http://www.zhone.com' + line.split('href="')[1].split('">')[0])
        for url in urls:
            http.get(url, None, 3)
            for line in http.content.split('\n'):
                if 'href="..' in line and '.zip' in line:
                    down = 'http://www.zhone.com/support/downloads/cpe'
                    down += line.split('href="..')[1].split('" onclick')[0]
                    downloads.append(down)
        try:
            os.makedirs('zhones')
        except:pass
        print len(downloads)
        for down in downloads:
            print down
            try:
                http.get(down, None, 3)
                name = down.split('/')[-1]
                f = open('zhones/%s' % name, 'w+')
                f.write(http.content)
                f.close()
            except Exception as e:
                print e

if __name__ == "__main__":
    scrape = Scrape()
    scrape.run()
